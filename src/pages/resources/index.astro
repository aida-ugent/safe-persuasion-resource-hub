---
import PageLayout from "@/layouts/Base.astro";

const meta = {
	title: "Resources",
	description: "Datasets, codebases, and research papers related to LLM persuasion",
};

type LinkWithUrl =
	| string
	| {
			url: string;
			label?: string;
	  };

type ResourceLink =
	| string
	| {
			url: string;
			description: string;
	  };

type ResourceItem = {
	title: string;
	year: string;
	authors: string[];
	description: string;
	paper: LinkWithUrl | null;
	studyWebsite: LinkWithUrl | null;
	dataset: ResourceLink | null;
	code: ResourceLink | null;
};

const getResourceUrl = (link: LinkWithUrl) => (typeof link === "string" ? link : link.url);

const getResourceLabel = (link: LinkWithUrl, fallback: string) =>
	typeof link === "string" ? fallback : link.label ?? fallback;

const getResourceDescription = (link: ResourceLink, fallback: string) =>
	typeof link === "string" ? fallback : link.description;

const getShortAuthors = (authors: string[]) => {
	if (authors.length === 0) return "";

	const firstAuthor = authors[0]?.trim() ?? "";
	const firstAuthorSurname = firstAuthor.split(" ").filter(Boolean).at(-1) ?? firstAuthor;

	return authors.length > 1 ? `${firstAuthorSurname} et al.` : firstAuthorSurname;
};

const resources: ResourceItem[] = [
	{
		title: "Persuading across Diverse Domains: A Dataset and Persuasion Large Language Model",
		year: "2024",
		authors: [
			"Chuhao Jin",
			"Kening Ren",
			"Lingzhen Kong",
			"Xiting Wang",
			"Ruihua Song",
			"Huan Chen",
		],
		description:
			"Introduces DailyPersuasion, a large-scale multi-domain persuasive dialogue dataset, and PersuGPT, a model specialized in persuasion strategies.",
		paper: "https://aclanthology.org/2024.acl-long.92/",
		studyWebsite: "https://persugpt.github.io",
		dataset: {
			url: "https://github.com/PersuGPT/PersuGPT.github.io/blob/main/DailyPersuasion_full_version.zip",
			description:
				"DailyPersuasion: A dataset covering 13,000 dialogue scenarios across 35 distinct domains.",
		},
		code: {
			url: "https://persugpt.github.io",
			description: "Code for the PersuGPT model and data collection framework.",
		},
	},
	{
		title:
			"Measuring and Benchmarking Large Language Modelsâ€™ Capabilities to Generate Persuasive Language",
		year: "2024",
		authors: ["Amalie Brogaard Pauli", "Isabelle Augenstein", "Ira Assent"],
		description: "A study of LLM ability to produce persuasive text.",
		paper: "https://arxiv.org/abs/2406.17753",
		studyWebsite: null,
		dataset: {
			url: "https://huggingface.co/datasets/APauli/Persuasive-Pairs",
			description:
				"<strong>Persuasive-Pairs</strong>: 2,700 text pairs from news, debates, and chats. Each includes an LLM-rewritten version with varying persuasiveness, validated by three human annotators.",
		},
		code: {
			url: "https://huggingface.co/APauli/Persuasive_language_in_pairs",
			description:
				"A trained regression model model to evaluate the relative persuasiveness between two text samples.",
		},
	},
	{
		title: "MakeMeSay, OpenAI o3-mini System Card",
		year: "2025",
		authors: ["OpenAI"],
		description: "",
		paper: "https://cdn.openai.com/o3-mini-system-card-feb10.pdf",
		studyWebsite: null,
		dataset: null,
		code: {
			url: "https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say",
			description:
				"This evaluation tests a modelâ€™s ability to generate persuasive or manipulative text, specifically in the setting of a game where one AI system has to get the other party (an AI system simulating a human) to say a specific codeword without arousing suspicion.",
		},
	},
	{
		title: "MakeMePay, OpenAI o3-mini System Card",
		year: "2025",
		authors: ["OpenAI"],
		description: "",
		paper: "https://cdn.openai.com/o3-mini-system-card-feb10.pdf",
		studyWebsite: null,
		dataset: null,
		code: {
			url: "https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay",
			description:
				"This evaluation tests an AI systemâ€™s ability to generate persuasive or manipulative text, specifically in the setting of convincing another (simulated) entity to part with money.",
		},
	},
	{
		title: "Among Them: A Game-Based Framework for Assessing Persuasion Capabilities of LLMs",
		year: "2025",
		authors: [
			"Mateusz Idziejczak",
			"Vasyl Korzavatykh",
			"Mateusz Stawicki",
			"Andrii Chmutov",
			"Marcin Korcz",
			"Iwo BÅ‚Ä…dek",
			"Dariusz Brzezinski",
		],
		description:
			"An evaluation framework using social deduction gameplay to measure how LLMs use persuasion and deception in dynamic environments.",
		paper: "https://arxiv.org/abs/2502.20426",
		studyWebsite: null,
		dataset: {
			url: "https://doi.org/10.5281/zenodo.14935395",
			description:
				"Logs and transcripts from simulated social deduction games used to analyze model behavior.",
		},
		code: {
			url: "https://github.com/Farmerobot/among_them",
			description:
				"A simulation platform inspired by 'Among Us' for testing persuasive and deceptive capabilities in LLMs.",
		},
	},
	{
		title:
			"Persuade Me if You Can: A Framework for Evaluating Persuasion Effectiveness and Susceptibility Among Large Language Models",
		year: "2025",
		authors: ["Nimet Beyza Bozdag", "Shuhaib Mehri", "Gokhan Tur", "Dilek Hakkani-TÃ¼r"],
		description:
			"The PMIYC framework evaluates LLMs in multi-turn conversations to measure both their effectiveness as persuaders and susceptibility as persuadees.",
		paper: "https://arxiv.org/abs/2503.01829",
		studyWebsite: "https://beyzabozdag.github.io/PMIYC",
		dataset: {
			url: "https://github.com/beyzabozdag/PersuadeMeIfYouCan/tree/main/data",
			description:
				"Benchmark data used to measure shifts in model and user opinions across controversial topics. The dataset comprises 961 subjective claims spanning political, ethical, and social issues sourced from Durmus et al. and the Perspectrum dataset, alongside 817 factual misinformation question-answer pairs adapted from the TruthfulQA benchmark.",
		},
		code: {
			url: "https://github.com/beyzabozdag/PersuadeMeIfYouCan",
			description: "Code for the multi-agent simulation framework.",
		},
	},
	{
		title: "Measuring and Improving Persuasiveness of Large Language Models",
		year: "2024",
		authors: ["Somesh Singh", "Yaman K Singla", "Harini SI", "Balaji Krishnamurthy"],
		description:
			"Introduces PersuasionBench and PersuasionArena to evaluate LLM generative and simulative persuasion capabilities, including the task of 'transsuasion'.",
		paper: "https://arxiv.org/abs/2410.02653",
		studyWebsite: "https://behavior-in-the-wild.github.io/measure-persuasion",
		dataset: {
			url: "https://huggingface.co/datasets/behavior-in-the-wild/PersuasionArena",
			description:
				"The PersuasionArena dataset for evaluating LLM persuasion across different tasks consists of tweet pairs where two tweets from the same account have similar content and were posted in close temporal proximity, but received significantly different engagement (e.g., number of likes). These differences act as a proxy for persuasiveness, allowing models to be trained and evaluated on generating or ranking more persuasive content.",
		},
		code: null,
	},
	{
		title:
			"Itâ€™s the Thought that Counts: Evaluating the Attempts of Frontier LLMs to Persuade on Harmful Topics",
		year: "2025",
		authors: ["Matthew Kowal", "Jasper Timm", "Jean-Francois Godbout", "others"],
		description:
			"The APE benchmark evaluates the propensity (willingness) of LLMs to attempt persuasion on harmful topics like conspiracies and violence.",
		paper: "https://arxiv.org/abs/2506.02873",
		studyWebsite: null,
		dataset: null,
		code: {
			url: "https://github.com/AlignmentResearch/AttemptPersuadeEval",
			description:
				"APE framework for measuring persuasion attempts. Includes topics, prompts and code for generating a synthetic dataset.",
		},
	},
	{
		title: "MultiAgentBench: Evaluating the Collaboration and Competition of LLM agents",
		year: "2025",
		authors: ["Kunlun Zhu", "Hongyi Du", "Zhaochen Hong", "others"],
		description:
			"Benchmarks multi-agent coordination and competition across scenarios like coding, research, and games (persuasion related tasks: Werewolf, Bargaining).",
		paper: "https://arxiv.org/abs/2503.01935",
		studyWebsite: null,
		dataset: null,
		code: {
			url: "https://github.com/MultiagentBench/MARBLE",
			description:
				"The MARBLE framework for multi-agent collaboration and competition evaluation. ",
		},
	},

	{
		title: "Werewolf Arena: A Case Study in LLM Evaluation via Social Deduction",
		year: "2024",
		authors: ["Suma Bailis", "Jane Friedhoff", "Feiyang Chen"],
		description:
			"A framework for evaluating LLMs via the social deduction game Werewolf, focusing on persuasion, deception, deduction.",
		paper: "https://arxiv.org/abs/2407.13943",
		studyWebsite: null,
		dataset: null,
		code: {
			url: "https://github.com/google/werewolf_arena",
			description: "Code and prompt templates for the Werewolf Arena simulation framework.",
		},
	},
	{
		title: "Measuring the Persuasiveness of Language Models",
		year: "2024",
		authors: [
			"Esin Durmus",
			"Liane Lovitt",
			"Alex Tamkin",
			"Stuart Ritchie",
			"Jack Clark",
			"Deep Ganguli",
		],
		description:
			"An Anthropic study measuring human belief shifts on various topics after reading arguments generated by Claude models.",
		paper: {
			url: "https://www.anthropic.com/news/measuring-model-persuasiveness",
			label: "Blog post",
		},
		studyWebsite: null,
		dataset: {
			url: "https://huggingface.co/datasets/Anthropic/persuasion",
			description:
				"The Persuasion Dataset contains claims and corresponding human-written and model-generated arguments, along with persuasiveness scores.",
		},
		code: null,
	},

	{
    title: "LLM Can Be a Dangerous Persuader: Empirical Study of Persuasion Safety in Large Language Models",
    year: "2025",
    authors: [
        "Minqian Liu",
        "Zhiyang Xu",
        "Xinyi Zhang",
        "Heajun An",
        "Sarvech Qadir",
        "Qi Zhang",
        "Pamela J. Wisniewski",
        "Jin-Hee Cho",
        "Sang Won Lee",
        "Ruoxi Jia",
        "Lifu Huang"
    ],
    description:
        "Introduces PersuSafety, a three-stage framework evaluating whether LLMs correctly refuse unethical persuasion tasks and avoid harmful persuasion strategies.",
    paper: "https://arxiv.org/abs/2504.10430",
    studyWebsite: null,
    dataset: null, // no public dataset link provided
    code: {
        url: "https://github.com/AlignmentResearch/AttemptPersuadeEval",
        description: "Code and evaluation setup for the PersuSafety persuasion safety benchmark."
    }
	},
	{
    title: "Communication Makes Perfect: Persuasion Dataset Construction via Multiâ€‘LLM Communication",
    year: "2025",
    authors: [
        "Weicheng Ma",
        "Hefan Zhang",
        "Ivory Yang",
        "Shiyu Ji",
        "Joice Chen",
        "Farnoosh Hashemi",
        "Shubham Mohole",
        "Ethan Gearey",
        "Michael Macy",
        "Saeed Hassanpour",
        "Soroush Vosoughi"
    ],
    description:
        "A multiâ€‘LLM communication framework for automatically generating largeâ€‘scale, highâ€‘quality persuasive dialogue datasets.",
    paper: "https://aclanthology.org/2025.naacl-long.203/",
    studyWebsite: null,
    dataset: null, // dataset is generated by framework, not separately hosted
    code: null // no dedicated repository publicly linked
},
{
    title: "â€œI understand your perspectiveâ€: LLM Persuasion through the Lens of Communicative Action Theory",
    year: "2026",
    authors: ["Esra DÃ¶nmez", "Agnieszka Falenska"],
    description:
        "Analyzes LLM persuasion strategies using Habermasâ€™ communicativeâ€‘action framework, focusing on illocutionary intent, alignment behavior, and humanâ€‘like persuasive communication.",
    paper: "https://aclanthology.org/2025.findings-acl.793/",
    studyWebsite: null,
    dataset: null,
    code: null
},
{
  title: "PersuasionForGood (P4G)",
  year: "2019",
  authors: ["Xuewei Wang", "Weiyan Shi", "Richard Kim", "Yoojung Oh", "Sijia Yang", "Jingwen Zhang", "Zhou Yu"],
  description:
    "Crowdsourced persuasion dialogues where a persuader aims to convince a partner to donate to Save the Children; 1,017 conversations (300 with sentenceâ€‘level persuasionâ€‘act annotations).",
  paper: "https://aclanthology.org/P19-1566/",
  studyWebsite: "https://convokit.cornell.edu/documentation/persuasionforgood.html",
  dataset: {
    url: "https://github.com/ohyj1002/persuasionforgood",
    description: "Original release with AnnotatedData (300) and FullData (1,017) dialogues."
  },
  code: {
    url: "https://gitlab.com/ucdavisnlp/persuasionforgood/-/tree/master?ref_type=heads",
    description: "Reference code and data organization from the original authors."
  }
},
{
  title: "UKPConvArg (Strict / Rank / All)",
  year: "2016â€“2020",
  authors: ["Ivan Habernal", "Iryna Gurevych", "Edwin Simpson", "others"],
  description:
    "Crowdsourced corpora of >16k argument pairs across 32 controversial topics with pairwise convincingness judgments; variants include pairwise ('Strict'), ranking and extended releases.",
  paper: "https://aclanthology.org/D16-1129.pdf",
  studyWebsite: "https://tudatalib.ulb.tu-darmstadt.de/items/4198f487-1b64-463b-a0ae-a678121f391f",
  dataset: {
    url: "https://github.com/UKPLab/acl2016-convincing-arguments",
    description: "Code + data (UKPConvArg1). UKPConvArg2 available via TU Darmstadt data portal."
  },
  code: {
    url: "https://arxiv.org/pdf/1806.02418v1.pdf",
    description: "Related Bayesian preference learning work on convincingness using UKPConvArg data."
  }
},
{
  title: "IBM-ArgQ (6.3kArgs & 14kPairs)",
  year: "2019",
  authors: ["Assaf Toledo", "Shai Gretz", "Edo Cohenâ€‘Karlik", "others"],
  description:
    "Actively collected 6.3k individual arguments with quality labels and 14k argument pairs with pairwise convincingness judgments, released by IBM Project Debater.",
  paper: "https://arxiv.org/pdf/1909.01007.pdf",
  studyWebsite: null,
  dataset: {
    url: "https://huggingface.co/datasets/ibm-research/argument_quality_ranking_30k",
    description: "IBM argument quality ranking dataset (related release) with ~30k items."
  },
  code: null
},
{
  title: "Winning-Arguments / ChangeMyView (CMV)",
  year: "2016",
  authors: ["Chenhao Tan", "Vlad Niculae", "Cristian Danescu-Niculescu-Mizil", "Lillian Lee"],
  description:
    "CMV conversations linking arguments to opinion change (Î”) outcomes; includes paired successful vs. matched unsuccessful threads.",
  paper: "http://chenhaot.com/pages/changemyview.html",
  studyWebsite: "https://convokit.cornell.edu/documentation/winning.html",
  dataset: {
    url: "https://chenhaot.com/data/cmv/README.txt",
    description: "Original CMV data and paired argument threads (training/heldâ€‘out)."
  },
  code: null
},
{
  title: "Webis-Clickbait-17",
  year: "2017",
  authors: ["Martin Potthast", "Tim Gollub", "Matti Wiegmann", "Benno Stein", "Matthias Hagen", "others"],
  description:
    "38,517 Twitter teaser posts from 27 US news publishers with 4â€‘point clickbait strength annotations; often used as a proxy for manipulative teaser design.",
  paper: "https://aclanthology.org/C18-1127/",
  studyWebsite: "https://webis.de/data/webis-clickbait-17.html",
  dataset: {
    url: "https://explore.openaire.eu/search/dataset?pid=10.5281%2Fzenodo.3346490",
    description: "Zenodo DOIs for training/test partitions via the Clickbait Challenge 2017."
  },
  code: null
},
{
  title: "Propaganda Techniques Corpus (PTC)",
  year: "2019â€“2020",
  authors: ["Giovanni Da San Martino", "Seunghak Yu", "Alberto BarrÃ³n-CedeÃ±o", "Rostislav Petrov", "Preslav Nakov"],
  description:
    "500+ news articles annotated at span level with 18 propaganda techniques; basis for SemEvalâ€‘2020 Task 11.",
  paper: "https://arxiv.org/abs/2003.11563",
  studyWebsite: "https://propaganda.math.unipd.it/ptc/",
  dataset: {
    url: "https://service.tib.eu/ldmservice/dataset/propaganda-techniques-corpus--ptc-",
    description: "Dataset hub + leaderboards; span identification and technique classification tasks."
  },
  code: {
    url: "https://huggingface.co/QCRI/PropagandaTechniquesAnalysis-en-BERT",
    description: "Reference models for fineâ€‘grained propaganda detection (BERT)."
  }
},
{
  title: "ElecDeb60to20",
  year: "2023â€“2025",
  authors: ["Pierpaolo Goffredo", "Mariana Chaves Espinoza", "Elena Cabrio", "Serena Villata"],
  description:
    "U.S. presidential debate transcripts (1960â€“2020) annotated for logical fallacies at the utterance/span level, plus argumentative components and relations.",
  paper: "https://openreview.net/pdf?id=Js80TDwMfY",
  studyWebsite: null,
  dataset: {
    url: "https://github.com/pierpaologoffredo/ElecDeb60to20",
    description: "Fallacy, argument component, and relationship annotations; debateâ€‘level data."
  },
  code: {
    url: "https://github.com/pierpaologoffredo/FallacyDetection",
    description: "MultiFusion BERT and baselines for fallacy detection/classification."
  }
},
{
  title: "Perspectrum",
  year: "2019",
  authors: ["Sihao Chen", "Daniel Khashabi", "Wenpeng Yin", "Chris Callison-Burch", "Dan Roth"],
  description:
    "Claims linked to diverse supporting/opposing perspectives and evidence paragraphs; stanceâ€‘aware perspective discovery dataset.",
  paper: "https://danielkhashabi.com/files/2019_perspectrum/2019_perspectrum_naacl.pdf",
  studyWebsite: "https://cogcomp.seas.upenn.edu/perspectrum/",
  dataset: {
    url: "https://github.com/CogComp/perspectrum",
    description: "Data, scripts, and demo for claims, perspectives, and evidence."
  },
  code: null
},
{
  title: "PersuasiveToM",
  year: "2025",
  authors: ["Fangxu Yu", "Lai Jiang", "Shenyi Huang", "Zhen Wu", "Xinyu Dai"],
  description:
    "Benchmark assessing Theory of Mind in persuasive dialogues: tracking desires/beliefs/intentions and applying ToM to select/evaluate persuasion strategies.",
  paper: "https://arxiv.org/abs/2502.21017",
  studyWebsite: null,
  dataset: {
    url: "https://github.com/Yu-Fangxu/PersuasiveToM",
    description: "Benchmark data and evaluation scripts."
  },
  code: {
    url: "https://github.com/Yu-Fangxu/PersuasiveToM",
    description: "Reference implementation and evaluation runners."
  }
},
{
  title: "PoliProp & PoliIssues (debate.org extended corpora)",
  year: "2018â€“2020",
  authors: ["Multiple (compiled)"],
  description:
    "Aggregations based on debate.org including propositions, arguments, and voter metadata (stances/demographics). Useful for political persuasion and stance modeling.",
  paper: null,
  studyWebsite: null,
  dataset: {
    url: null,
    description: "Note: Public mirrors move; cite the originating debate.orgâ€‘derived compilations used in your work."
  },
  code: null
},
{
  title: "TeleSalesCorpus",
  year: "2025",
  authors: ["â€”"],
  description:
    "Synthetic, multiâ€‘turn, goalâ€‘oriented sales dialogues (~2,000 conversations) for studying persuasive tactics in teleâ€‘sales settings.",
  paper: null,
  studyWebsite: null,
  dataset: {
    url: null,
    description: "Add your release link here; several synthetic sales corpora exist with similar specs."
  },
  code: null
},
{
  title: "CToMPersu",
  year: "2025",
  authors: ["â€”"],
  description:
    "Multiâ€‘domain persuasion scenarios explicitly annotated for information asymmetry and Theoryâ€‘ofâ€‘Mind variables.",
  paper: null,
  studyWebsite: null,
  dataset: { url: null, description: "Awaiting canonical link/publication." },
  code: null
},
{
  title: "EPP4G & ETP4G (PersuasionForGood extensions)",
  year: "2020â€“2024",
  authors: ["â€”"],
  description:
    "Extended P4G annotations with emotion labels (EPP4G), politenessâ€‘strategy labels, and empathetic transfer tags (ETP4G).",
  paper: null,
  studyWebsite: null,
  dataset: { url: null, description: "Add links to the extended annotated releases you use." },
  code: null
},
{
  title: "ImageArg",
  year: "â€”",
  authors: ["â€”"],
  description:
    "A multiâ€‘modal Twitter dataset targeting image persuasiveness in short social posts.",
  paper: null,
  studyWebsite: null,
  dataset: { url: null, description: "Please provide the canonical link for ImageArg; several â€˜image persuasionâ€™ datasets exist under similar names." },
  code: null
},
{
  title: "3MT French (Persuasiveness Ratings)",
  year: "â€”",
  authors: ["â€”"],
  description:
    "Crowd evaluations of persuasiveness for 3â€‘minute academic presentations by French PhD students.",
  paper: null,
  studyWebsite: null,
  dataset: { url: null, description: "Awaiting authoritative dataset link/paper to finalize." },
  code: null
},


	// {
	// 	title: "",
	// 	year: "",
	// 	authors: [],
	// 	description: "",
	// 	paper: "",
	// 	studyWebsite: null,
	// 	dataset: {
	// 		url: "",
	// 		description: "",
	// 	},
	// 	code: {
	// 		url: "",
	// 		description: "",
	// 	},
	// },
	// {
	// 	title: "",
	// 	year: "",
	// 	authors: [],
	// 	description: "",
	// 	paper: "",
	// 	studyWebsite: null,
	// 	dataset: {
	// 		url: "",
	// 		description: "",
	// 	},
	// 	code: {
	// 		url: "",
	// 		description: "",
	// 	},
	// },
	// {
	// 	title: "",
	// 	year: "",
	// 	authors: [],
	// 	description: "",
	// 	paper: "",
	// 	studyWebsite: null,
	// 	dataset: {
	// 		url: "",
	// 		description: "",
	// 	},
	// 	code: {
	// 		url: "",
	// 		description: "",
	// 	},
	// },

];

const withBase = (path: string) => `${import.meta.env.BASE_URL.replace(/\/$/, "")}${path}`;
---

<PageLayout meta={meta}>
	<h1 class="title mb-8">Resources</h1>

	<!-- <section class="prose prose-sm prose-cactus mb-8 max-w-none">
		<p>
			Datasets, codebases, and research related to persuasion and behavior change in LLMs. Each
			entry may include papers, datasets, code, or a combination. Use the badges below each entry to
			jump to specific resources.
		</p>
	</section> -->

	<div class="space-y-8">
		{
			resources.map((resource) => (
				<div class="border-accent/30 bg-opacity-5 rounded-lg border p-6">
					<h3 class="title mb-2 text-lg">{resource.title}</h3>
					{(resource.studyWebsite ||
						resource.paper ||
						resource.year ||
						resource.authors.length > 0) && (
						<div class="mb-4 flex flex-wrap items-center gap-3 text-sm opacity-80">
							{resource.studyWebsite && (
								<a
									href={getResourceUrl(resource.studyWebsite)}
									target="_blank"
									rel="noreferrer"
									class="border-accent/60 text-accent inline-block flex-shrink-0 rounded border px-3 py-1 text-xs font-semibold"
								>
									ðŸ”— Website
								</a>
							)}
							{resource.paper && (
								<a
									href={getResourceUrl(resource.paper)}
									target="_blank"
									rel="noreferrer"
									class="border-accent/60 text-accent inline-block flex-shrink-0 rounded border px-3 py-1 text-xs font-semibold"
								>
									ðŸ“„ {getResourceLabel(resource.paper, "Paper")}
								</a>
							)}
							{(resource.year || resource.authors.length > 0) && (
								<p>
									{resource.year}
									{resource.year && resource.authors.length > 0 && " â€¢ "}
									{getShortAuthors(resource.authors)}
								</p>
							)}
						</div>
					)}
					<p class="mb-4 text-sm">{resource.description}</p>
					<div class="space-y-3">
						{resource.dataset && (
							<div class="flex items-start gap-3">
								<a
									href={getResourceUrl(resource.dataset)}
									target="_blank"
									rel="noreferrer"
									class="bg-accent mt-0.5 inline-flex w-28 flex-shrink-0 justify-center rounded px-3 py-1 text-xs font-semibold text-white dark:text-black"
								>
									ðŸ“Š Dataset
								</a>
								<p
									class="text-sm"
									set:html={getResourceDescription(resource.dataset, "Link to the dataset.")}
								/>
							</div>
						)}
						{resource.code && (
							<div class="flex items-start gap-3">
								<a
									href={getResourceUrl(resource.code)}
									target="_blank"
									rel="noreferrer"
									class="bg-accent mt-0.5 inline-flex w-28 flex-shrink-0 justify-center rounded px-3 py-1 text-xs font-semibold text-white dark:text-black"
								>
									ðŸ’» Code
								</a>
								<p class="text-sm">
									{getResourceDescription(resource.code, "Link to the code repository.")}
								</p>
							</div>
						)}
					</div>
				</div>
			))
		}
	</div>

	<section class="prose prose-sm prose-cactus mt-12 max-w-none">
		<h3>How to contribute</h3>
		<p>
			If you know of a relevant resource that is missing, there are two ways to add it:
			<ul class="list-inside list-disc">
				<li>
					<strong>Contact us</strong> â€” email us the resource information, details in the <a
						href={withBase("/contact/")}>Contact</a
					> page.
				</li>
				<li>
					<strong>GitHub</strong> â€” add it to the website's <a
						href="https://github.com/aida-ugent/llm-persuasion-safety-hub"
						target="_blank"
						rel="noreferrer">GitHub repository</a
					> and raise a pull request.
				</li>
			</ul>
		</p>
	</section>
</PageLayout>
