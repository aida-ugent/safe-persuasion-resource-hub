---
import PageLayout from "@/layouts/Base.astro";

const meta = {
	description: "About the Safe Persuasion Hub",
	title: "About",
};

const withBase = (path: string) => `${import.meta.env.BASE_URL.replace(/\/$/, "")}${path}`;
---

<PageLayout meta={meta}>
	<h1 class="title mb-8">About</h1>
	<div class="prose prose-sm prose-cactus max-w-none">
		<p>
			The <strong>Safe Persuasion Hub</strong> is a curated collection of datasets, codebases, and research
			papers related to the evaluation of persuasive capabilities in large language models (LLMs). The
			hub focuses on measurement, analysis, and safety-relevant research rather than the development of
			persuasion techniques.
		</p>
		<h3>Why this hub?</h3>
		<p>
			Work on LLM persuasion spans multiple disciplines—including dialogue systems, behavioral
			science, rhetoric, and AI safety—and relevant resources are often scattered across GitHub,
			arXiv, Hugging Face, and institutional repositories. This hub aims to provide a single,
			structured entry point for researchers, auditors, and policymakers.
		</p>
		<h3>What we include</h3>
		<ul class="list-inside list-disc">
			<li>
				<strong>Datasets</strong> — text and dialogue corpora, annotated persuasion examples, preference
				and evaluation data
			</li>
			<li>
				<strong>Codebases</strong> — model implementations, fine-tuning tools, and evaluation frameworks
			</li>
			<li>
				<strong>Research papers</strong> — with links to associated code and data when available
			</li>
		</ul>
		<h3>How to contribute</h3>
		<p>
			If you know of a relevant resource, please <a href={withBase("/contact/")}>contact us</a> or submit
			an issue or pull request on our <a
				href="https://github.com/aida-ugent/llm-persuasion-safety-hub"
				target="_blank"
				rel="noreferrer">GitHub repository</a
			>.
		</p>
	</div>
</PageLayout>
